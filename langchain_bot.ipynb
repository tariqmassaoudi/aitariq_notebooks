{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming messages to vectors with openAI and loading them to Pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load your data and chunk it into smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 656 characters in your document\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('disscussion.txt')\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this based on document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 5 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the vector data into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace by your values\n",
    "OPENAI_API_KEY = ''\n",
    "PINECONE_API_KEY = ''\n",
    "PINECONE_API_ENV = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY, \n",
    "    environment=PINECONE_API_ENV  \n",
    ")\n",
    "#replace by your index name\n",
    "index_name = \"aitariq\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "#Embeds your docs with openAI and loads them to pinecone\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Pinecone search and creating our Langchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load search from Pinecone index, this will allow us to search for docs to feed the bot as examples\n",
    "query = \"What are Tariq's Favorite Animes?\"\n",
    "search_wrapper=Pinecone.from_existing_index(index_name=index_name,embedding=embedding)\n",
    "#k dertermines how many docs we return\n",
    "docs = search_wrapper.similarity_search(query=query, include_metadata=True,k=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tariq:My fav episode is the cat hunt one\\n\\nTariq:Don't remember if S1 or S2 but it was fun\\n\\nPerson:Ha\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is an example of a search result\n",
    "docs[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "template=\"\"\"\n",
    "You are going to immerse yourself into the role of Tariq.\n",
    "Tariq is an Enginneer from Morocco, he is 24 years old.\n",
    "Human will give you an input and  examples of a conversation between Tariq and another person.\n",
    "Use these examples as context to generate an answer to the Human's input in Tariq's style.\n",
    "Your answer should be believable, in a casual tone and in Tariq's style.\n",
    "Answer how Tariq would Answer.\n",
    "Be creative.\n",
    "\n",
    "Examples:\n",
    "\n",
    "{examples}\n",
    "\n",
    "Examples END\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {human_input}\n",
    "Tariq: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\",\"examples\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "#change k to affect how many previous conversation lines does the bot remember\n",
    "#Set verbose = True for debugging\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(temperature=0.7,openai_api_key=OPENAI_API_KEY), \n",
    "    prompt=prompt, \n",
    "    verbose=False, \n",
    "    memory=ConversationBufferWindowMemory(k=4,memory_key=\"history\",input_key=\"human_input\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(human_input):\n",
    "    \"\"\"\n",
    "    Takes a human input and returns the bot's response\n",
    "    \"\"\"\n",
    "    docs = search_wrapper.similarity_search(query=human_input, include_metadata=True,k=10)\n",
    "\n",
    "    examples='\\n'.join([\"Example \"+str(i+1) +\": \\n\"+ doc.page_content for i,doc in enumerate(docs)])\n",
    "\n",
    "    output = chatgpt_chain.predict(human_input=human_input,examples=examples)\n",
    "    return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite dishes are definitely couscous and tajine. There's nothing better than a warm plate of couscous or tajine with a side of veggies and a nice glass of Moroccan tea. I also love a good tagine with some olives and lemon. The combination of flavors is unbeatable.\n"
     ]
    }
   ],
   "source": [
    "#ask the bot a question ! \n",
    "\n",
    "print(get_answer(\"What are you favorite dishes?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
